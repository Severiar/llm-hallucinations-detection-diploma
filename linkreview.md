| Название | Год | Автор | Ссылка | Краткое содержание |
| -------- |---- | ----- | ------ | ---- |
|Developing a Reliable, General-Purpose Hallucination Detection and Mitigation Service: Insights and Lessons Learned|2024|Song Wang, Xun Wang, Jie Mei, Yujia Xie, Sean Muarray, Zhang Li, Lingfeng Wu, Si-Qing Chen, Wayne Xiong|[link](https://arxiv.org/abs/2407.15441)|Два метода на основе SMBO и критерия Expected Improvement: GP - моделирование $p(y\|x)$ с помощью гауссовского процесса; TPE - моделирование $p(x\|y)$ и $p(x)$, построение распределений гиперпараметров и сэмплирование новых, максимизирующих $\frac{l(x)}{g(x)}$|
|Sequential Model-Based Optimization for General Algorithm Configuration|2011|Hutter, et al.|[link](https://ml.informatik.uni-freiburg.de/wp-content/uploads/papers/11-LION5-SMAC.pdf)|Описывают SMBO, вводят процедуру Intensify, которая выбирает оптимальный HPC на основе множества датасетов, представляют SMAC - SMBO с RF в качестве суррогата для поддержи категориальных переменных|
|Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization|2016|Li et al.|[link](https://arxiv.org/abs/1603.06560)|Запускает Succesive Halving много раз, постоянно увеличивая бюджет для лучших моделей|
|Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges|2021|Bischl et al.|[link](https://arxiv.org/abs/2107.05847)|Обзорная статья о текущих методах HPO, все алгоритмы формально описаны. Рассказано о текущих проблемах в области HPO, таких как overtuning и отсутствие регуляризации|
|Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets|2016|Klein et al.|[link](https://arxiv.org/abs/1605.07079)|Добавили к пространству гиперпараметров новый параметр $s \in (0, 1]$, означающий долю размера датасета. Таким образом, используя SMBO с GP, можно обучать модель на меньшем датасете и экстраполировать значение ошибки на валидации для $s=1$|
|Scalable Bayesian Optimization Using Deep Neural Networks|2015| Snoek et al.|[link](https://arxiv.org/pdf/1502.05700.pdf)|SMBO, суррогат - NN + Bayesian Linear Regression. Быстрее чем GP (рост линейный от количества тестов)|
|Optimization of deep neural networks: a survey and unified taxonomy|2020|El-Ghazali Talbi|[link](https://inria.hal.science/hal-02570804v2/document)|Обзорная статья, систематизация знаний о HPO, NAS и AutoDNN|
|BOHB: Robust and Efficient Hyperparameter Optimization at Scale|2018|Falkner et al.|[link](https://arxiv.org/abs/1807.01774)|Комбинация TPE и идеи HyperBand: HPC предлагаются с помощью TPE, $\lambda_{fid}$ повышается для новых HPC|
|Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces|1997|R. Storn, K. Price |[link](https://link.springer.com/article/10.1023/A:1008202821328)|Эволюционный алгоритм, каждый вектор из поколения мутируется кроссинговером с линейной комбинацией 3 случайных векторов|
|DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization|2021|Noor Awad, Neeratyoy Mallik, Frank Hutter|[link](https://arxiv.org/abs/2105.09821)|Комбинация DE и идеи HyperBand: в каждой итерации внутри bracket новые поколения предлагаются с помощью DE и вычисляются с увеличенным бюджетом, при этом векторы для мутации сэмплируются не из текущего поколения, а из лучших векторов предыдущей SH bracket|
|An empirical evaluation of deep architectures on problems with many factors of variation|2007|Larochelle et al.|[link](https://dl.acm.org/doi/10.1145/1273496.1273556)|Сравниваются модели глубокого обучения с классическими ML моделями, авторы используют для этого grid search|
|Random Search for Hyper-Parameter Optimization|2012|Bergstra et al.|[link](https://jmlr.org/papers/v13/bergstra12a.html)|Авторы предлагают использовать случайный поиск вместо перебора по сетке, так как в многомерных задачах перебор по сетке многократно переиспользует одни и те же значения каждого гиперпараметра|
|Online Hyperparameter Optimization for Class-Incremental Learning|2023|Liu et al.|[link](https://ojs.aaai.org/index.php/AAAI/article/view/26070)|Используется гиперпараметрическую оптимизацию в задаче инкрементального по классам обучения. Задача оптимизации гиперпараметров преобразуется в марковский процесс принятия решений.|
|A survey on multi-objective hyperparameter optimization algorithms for machine learning|2022|Morales-Hernandez et al.|[link](https://link.springer.com/article/10.1007/s10462-022-10359-2)|Обзор прогресса в области многозадачной оптимизации гиперпараметров|
|Scalable Gaussian process-based transfer surrogates for hyperparameter optimization|2018|Wistuba et al.|[link](https://link.springer.com/article/10.1007/s10994-017-5684-y)|В статье предлагаются суррогаты на основе гауссовских процессов для SMBO|
|Non-stochastic Best Arm Identification and Hyperparameter Optimization|2015|K. Jamieson, A. Talwalkar|[link](https://arxiv.org/abs/1502.07943)|Предлагается алгоритм Succesive Halving, главная идея - постепенное увеличение бюджета вычислений для перспективных наборов параметров|
|SMAC3: A versatile bayesian optimization package for hyperparameter optimization|2023|Lindauer et a.|[link](https://arxiv.org/abs/2109.09831)|Новый auto-ml фреймворк для работы с задачей гиперпараметрической оптимизации. В нём присутствует большое число SMBO моделей и моделей, поддерживающих black-box оптимизацию и multiobjective оптимизацию|
|Scalable hyperparameter optimization with products of gaussian process experts|2016|Schilling et al.|[link](https://link.springer.com/chapter/10.1007/978-3-319-46128-1_3)|Авторы декомпозируют суррогат на основе гауссовского процесса на произведение множества суррогатов|
|Optimizing deep learning hyper-parameters through an evolutionary algorithm|2015|Young et al.|[link](https://www.researchgate.net/publication/301463804_Optimizing_deep_learning_hyper-parameters_through_an_evolutionary_algorithm)|Эволюционный алгоритм для оптимизации гиперпараметров глубокой нейронной сети|
|Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization|2021|H. Alibrahim, S. Ludwig|[link](http://cs.ndsu.edu/~siludwig/Publish/papers/CEC2021.pdf)|Сравнение grid search и байесовских подходов с генетическим алгоритмом|
|NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search|2020|Zela et al.|[link](https://arxiv.org/abs/2001.10422)|Новый бенчмарк для оценки гиперпараметрической оптимизации и поиска архитектур нейронных сетей|
