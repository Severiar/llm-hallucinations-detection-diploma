{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9940562,"sourceType":"datasetVersion","datasetId":5844045}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\nimport torch\n\n# Загрузка предобученной модели и токенизатора\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-12T16:35:51.748201Z","iopub.execute_input":"2024-12-12T16:35:51.748944Z","iopub.status.idle":"2024-12-12T16:36:12.962304Z","shell.execute_reply.started":"2024-12-12T16:35:51.748914Z","shell.execute_reply":"2024-12-12T16:36:12.961408Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a7350fa7aa42feae46d19b08ff7143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50181d027ac045d5a5bf0044c4cdbe97"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8533752dd1b400bab1a2d2ef5db3ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40007af433214210b3beb3cd918893d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dd2a1931ef48aa9925d9219d0abf78"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import DistilBertTokenizerFast\n\nfrom sklearn.model_selection import train_test_split\n\n# Загрузка данных\nwith open(\"/kaggle/input/semeval-2025-task-3-mu-shroom-dataset/qa_data_output.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# Разделение на train и val в отношении 9:1\ntrain_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n\n# Проверка размеров\nprint(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}\")\n\n# Сохранение данных для проверки\nwith open(\"train_data.json\", \"w\", encoding=\"utf-8\") as train_file:\n    json.dump(train_data, train_file, ensure_ascii=False, indent=4)\n\nwith open(\"val_data.json\", \"w\", encoding=\"utf-8\") as val_file:\n    json.dump(val_data, val_file, ensure_ascii=False, indent=4)\n\n# Инициализация токенизатора\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\nimport torch\n\nimport torch\n\ndef create_dataset(data, tokenizer, max_length=512):\n    input_ids = []\n    attention_masks = []\n    labels_list = []\n    offset_mappings = []  # To store the offset mappings\n\n    for item in data:\n        # Конкатенация question и hallucinated_answer\n        text = 'query: ' + item[\"question\"] + \"\\n answer: \" + item[\"hallucinated_answer\"]\n        if 'hallucination' not in item.keys():\n            continue\n        hallucinations = item[\"hallucination\"].split('\\n')  # Разделение галлюцинаций\n        hallucinations = [h.strip(\"- \").strip('\"').strip() for h in hallucinations]  # Убираем маркеры списка и пробелы\n\n        # Токенизация текста\n        encoding = tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length,\n            return_offsets_mapping=True,\n            return_tensors=\"pt\"\n        )\n\n        # Метки (все токены по умолчанию O: 0)\n        labels = [0] * max_length  # 0: O, 1: B-HALLUCINATION, 2: I-HALLUCINATION\n        offsets = encoding[\"offset_mapping\"][0].tolist()\n\n        # Проставление меток для каждой галлюцинации\n        for hallucination in hallucinations:\n            hallucination_start = text.rfind(hallucination)\n            if hallucination_start == -1:\n                continue  # Если галлюцинация не найдена в тексте\n            hallucination_end = hallucination_start + len(hallucination)\n\n            for idx, (offset_start, offset_end) in enumerate(offsets):\n                if offset_start >= hallucination_start and offset_end <= hallucination_end:\n                    if labels[idx] == 0:\n                        labels[idx] = 1  # B-HALLUCINATION\n                    else:\n                        labels[idx] = 2  # I-HALLUCINATION\n\n        # Учитываем padding токены\n        for idx, mask_value in enumerate(encoding[\"attention_mask\"][0].tolist()):\n            if mask_value == 0:\n                labels[idx] = 0  # Метка для padding токенов\n\n        # Сохраняем токены, маску и метки\n        input_ids.append(encoding[\"input_ids\"][0])\n        attention_masks.append(encoding[\"attention_mask\"][0])\n        labels_list.append(torch.tensor(labels))\n        offset_mappings.append(offsets)  # Save the offsets\n\n    return {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention_masks),\n        \"labels\": torch.stack(labels_list),\n        \"offset_mappings\": offset_mappings  # Return offset mappings\n    }\n\n\n\n# Создание обучающего набора\ntrain_dataset = create_dataset(train_data, tokenizer)\nval_dataset = create_dataset(val_data, tokenizer)\n\n# Проверка размеров\nprint(f\"Input IDs shape: {train_dataset['input_ids'].shape}\")\nprint(f\"Attention mask shape: {train_dataset['attention_mask'].shape}\")\nprint(f\"Labels shape: {train_dataset['labels'].shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-12T16:36:12.964002Z","iopub.execute_input":"2024-12-12T16:36:12.965041Z","iopub.status.idle":"2024-12-12T16:36:28.378971Z","shell.execute_reply.started":"2024-12-12T16:36:12.964998Z","shell.execute_reply":"2024-12-12T16:36:28.378070Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train size: 9000, Validation size: 1000\nInput IDs shape: torch.Size([8905, 512])\nAttention mask shape: torch.Size([8905, 512])\nLabels shape: torch.Size([8905, 512])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass QADataset(Dataset):\n    def __init__(self, dataset):\n        self.input_ids = dataset[\"input_ids\"]\n        self.attention_mask = dataset[\"attention_mask\"]\n        self.labels = dataset[\"labels\"]\n        self.offset_mappings = dataset[\"offset_mappings\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.input_ids) or idx < 0:\n            raise IndexError(f\"Invalid index: {idx}\")\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx],\n            \"offset_mappings\": self.offset_mappings[idx]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:36:49.096663Z","iopub.execute_input":"2024-12-12T16:36:49.096997Z","iopub.status.idle":"2024-12-12T16:36:49.103326Z","shell.execute_reply.started":"2024-12-12T16:36:49.096969Z","shell.execute_reply":"2024-12-12T16:36:49.102409Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",          # Папка для сохранения моделей\n    num_train_epochs=3,              # Количество эпох\n    per_device_train_batch_size=16,  # Размер батча\n    per_device_eval_batch_size=16,   # Размер батча для валидации\n    warmup_steps=500,                # Количество шагов для прогрева\n    weight_decay=0.01,               # Коэффициент L2-регуляризации\n    logging_dir=\"./logs\",            # Папка для логов\n    evaluation_strategy=\"epoch\",     # Валидация после каждой эпохи\n    save_strategy=\"epoch\",           # Сохранение модели после каждой эпохи\n    logging_steps=10,                # Логирование каждые 10 шагов\n    load_best_model_at_end=True      # Загрузка лучшей модели в конце\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=QADataset(train_dataset),\n    eval_dataset=QADataset(val_dataset)\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-12T16:36:52.615577Z","iopub.execute_input":"2024-12-12T16:36:52.616247Z","iopub.status.idle":"2024-12-12T16:36:53.927817Z","shell.execute_reply.started":"2024-12-12T16:36:52.616217Z","shell.execute_reply":"2024-12-12T16:36:53.926876Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:30:10.195092Z","iopub.execute_input":"2024-11-21T08:30:10.195790Z","iopub.status.idle":"2024-11-21T08:42:59.863695Z","shell.execute_reply.started":"2024-11-21T08:30:10.195757Z","shell.execute_reply":"2024-11-21T08:42:59.862887Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111301841111122, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a942848c81f44e3baafa4e0cde917afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241121_083018-zqx900mu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1671' max='1671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1671/1671 12:37, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.523400</td>\n      <td>0.537923</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.408100</td>\n      <td>0.512118</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.230100</td>\n      <td>0.676637</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1671, training_loss=0.4508122793130715, metrics={'train_runtime': 768.726, 'train_samples_per_second': 34.752, 'train_steps_per_second': 2.174, 'total_flos': 3490460678016000.0, 'train_loss': 0.4508122793130715, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:43:33.673866Z","iopub.execute_input":"2024-11-21T08:43:33.674252Z","iopub.status.idle":"2024-11-21T08:43:34.342172Z","shell.execute_reply.started":"2024-11-21T08:43:33.674211Z","shell.execute_reply":"2024-11-21T08:43:34.341052Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./trained_model/tokenizer_config.json',\n './trained_model/special_tokens_map.json',\n './trained_model/vocab.txt',\n './trained_model/added_tokens.json',\n './trained_model/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:44:54.396059Z","iopub.execute_input":"2024-11-21T08:44:54.396957Z","iopub.status.idle":"2024-11-21T08:45:03.005936Z","shell.execute_reply.started":"2024-11-21T08:44:54.396921Z","shell.execute_reply":"2024-11-21T08:45:03.005086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [62/62 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.5121184587478638, 'eval_runtime': 8.6005, 'eval_samples_per_second': 114.993, 'eval_steps_per_second': 7.209, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom datetime import datetime\n\n# Получение текущей даты и времени\ncurrent_datetime = datetime.now()\n\ntraining_args = TrainingArguments(\n    run_name=f\"RUN {current_datetime}\",\n    output_dir=f\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,           # Запуск валидации каждые 500 шагов\n    logging_steps=5,        # Логи каждые 500 шагов\n    save_strategy=\"steps\",    # Сохранение модели каждые 500 шагов\n    save_steps=500,\n    warmup_steps=500,                # Количество шагов для прогрева\n    weight_decay=0.01,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    load_best_model_at_end=True,  # Загрузка лучшей модели в конце\n    #metric_for_best_model=\"f1_macro\",  # Метрика для выбора лучшей модели\n    #greater_is_better=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:28.987892Z","iopub.execute_input":"2024-12-12T16:37:28.988232Z","iopub.status.idle":"2024-12-12T16:37:29.017728Z","shell.execute_reply.started":"2024-12-12T16:37:28.988201Z","shell.execute_reply":"2024-12-12T16:37:29.016760Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import f1_score, jaccard_score\n\ndef compute_metrics(pred, attention_mask):\n    predictions, labels = pred\n    \n    # Предсказания и метки\n    predictions = np.argmax(predictions, axis=2)\n\n    # Убираем padding токены на основе attention_mask\n    true_predictions = [\n        [p for (p, m) in zip(prediction, mask) if m == 1]\n        for prediction, mask in zip(predictions, attention_mask)\n    ]\n    true_labels = [\n        [l for (l, m) in zip(label, mask) if m == 1]\n        for label, mask in zip(labels, attention_mask)\n    ]\n\n    # Переводим списки в одномерные массивы\n    true_predictions = [item for sublist in true_predictions for item in sublist]\n    true_labels = [item for sublist in true_labels for item in sublist]\n\n    # Метрики\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:53.824971Z","iopub.execute_input":"2024-12-12T16:37:53.825643Z","iopub.status.idle":"2024-12-12T16:37:53.832055Z","shell.execute_reply.started":"2024-12-12T16:37:53.825609Z","shell.execute_reply":"2024-12-12T16:37:53.831109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import Trainer\nimport wandb\nwandb.init(project='your_project_name', name='new_run_name')  # Set unique name for each run\n\ndel model\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=QADataset(train_dataset),\n    eval_dataset=QADataset(val_dataset),\n    #compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:57.132395Z","iopub.execute_input":"2024-12-12T16:37:57.133077Z","iopub.status.idle":"2024-12-12T16:38:07.212528Z","shell.execute_reply.started":"2024-12-12T16:37:57.133042Z","shell.execute_reply":"2024-12-12T16:38:07.211870Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111276015555581, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62eaefac118c4ddaadfc11f3081b6af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241212_163803-3ltlp30w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w' target=\"_blank\">new_run_name</a></strong> to <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w</a>"},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:38:07.213741Z","iopub.execute_input":"2024-12-12T16:38:07.213980Z","iopub.status.idle":"2024-12-12T16:46:38.945640Z","shell.execute_reply.started":"2024-12-12T16:38:07.213940Z","shell.execute_reply":"2024-12-12T16:46:38.944858Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1114' max='1114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1114/1114 08:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.009800</td>\n      <td>0.011726</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.011000</td>\n      <td>0.010699</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1114, training_loss=0.05715645355451685, metrics={'train_runtime': 509.9673, 'train_samples_per_second': 34.924, 'train_steps_per_second': 2.184, 'total_flos': 2326973785344000.0, 'train_loss': 0.05715645355451685, 'epoch': 2.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Evaluate on the validation dataset\neval_dataloader = DataLoader(QADataset(val_dataset), batch_size=16)\n\n# Collect predictions and labels\npredictions = []\nlabels = []\nattention_masks = []\n\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        # Get batch data\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        label_ids = batch[\"labels\"].to(device)\n\n        # Make predictions\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions.append(logits.detach().cpu().numpy())\n        labels.append(label_ids.detach().cpu().numpy())\n        attention_masks.append(attention_mask.detach().cpu().numpy())\n\n# Combine batches into arrays\npredictions = np.concatenate(predictions, axis=0)\nlabels = np.concatenate(labels, axis=0)\nattention_masks = np.concatenate(attention_masks, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:55:09.936116Z","iopub.execute_input":"2024-12-12T16:55:09.936785Z","iopub.status.idle":"2024-12-12T16:55:19.221274Z","shell.execute_reply.started":"2024-12-12T16:55:09.936752Z","shell.execute_reply":"2024-12-12T16:55:19.220580Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def compute_metrics(pred, attention_mask, dataset):\n    predictions, labels = pred\n\n    # Predicted labels\n    predictions = np.argmax(predictions, axis=2)\n\n    # Filter predictions and labels using attention_mask\n    true_predictions = []\n    true_labels = []\n\n    for i, (prediction, label, mask) in enumerate(zip(predictions, labels, attention_masks)):\n        # Filter by attention mask\n        filtered_preds = [p for p, m in zip(prediction, mask) if m == 1]\n        filtered_labels = [l for l, m in zip(label, mask) if m == 1]\n\n        # Find start index of 'answer: ' for further filtering\n        text = tokenizer.decode(dataset[i][\"input_ids\"], skip_special_tokens=True)\n        answer_start_index = text.find(\"answer: \") + len(\"answer: \")\n\n        # Apply additional filtering for tokens after 'answer: '\n        offsets = dataset[i][\"offset_mappings\"]\n        filtered_preds = [\n            p for p, (start, _) in zip(filtered_preds, offsets) if start >= answer_start_index\n        ]\n        filtered_labels = [\n            l for l, (start, _) in zip(filtered_labels, offsets) if start >= answer_start_index\n        ]\n\n        true_predictions.extend(filtered_preds)\n        true_labels.extend(filtered_labels)\n\n    # Compute metrics\n    print(set(true_labels), set(true_predictions))\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:14:51.804635Z","iopub.execute_input":"2024-12-12T17:14:51.805310Z","iopub.status.idle":"2024-12-12T17:14:51.813305Z","shell.execute_reply.started":"2024-12-12T17:14:51.805278Z","shell.execute_reply":"2024-12-12T17:14:51.812375Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, jaccard_score\n\ndef compute_metrics(pred, attention_mask, dataset):\n    predictions, labels = pred\n\n    # Predicted labels\n    predictions = np.argmax(predictions, axis=2)\n\n    # Filter predictions and labels using attention_mask\n    true_predictions = []\n    true_labels = []\n\n    for i, (prediction, label, mask) in enumerate(zip(predictions, labels, attention_mask)):\n        # Filter by attention mask\n        filtered_preds = [p for p, m in zip(prediction, mask) if m == 1]\n        filtered_labels = [l for l, m in zip(label, mask) if m == 1]\n\n        # Find start index of 'answer: ' for further filtering\n        text = tokenizer.decode(dataset[i][\"input_ids\"], skip_special_tokens=True)\n        answer_start_index = text.find(\"answer: \") + len(\"answer: \")\n\n        # Apply additional filtering for tokens after 'answer: '\n        offsets = dataset[i][\"offset_mappings\"]\n        filtered_preds = [\n            p for p, (start, _) in zip(filtered_preds, offsets) if start >= answer_start_index\n        ]\n        filtered_labels = [\n            l for l, (start, _) in zip(filtered_labels, offsets) if start >= answer_start_index\n        ]\n\n        true_predictions.extend(filtered_preds)\n        true_labels.extend(filtered_labels)\n\n    # Compute overall metrics\n    true_labels = [min(1, i) for i in true_labels]\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    # Detailed class-specific metrics\n    class_report = classification_report(true_labels, true_predictions, output_dict=True)\n    print(classification_report(true_labels, true_predictions))\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n        \"class_report\": class_report,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:26:13.676674Z","iopub.execute_input":"2024-12-12T17:26:13.677516Z","iopub.status.idle":"2024-12-12T17:26:13.689853Z","shell.execute_reply.started":"2024-12-12T17:26:13.677467Z","shell.execute_reply":"2024-12-12T17:26:13.689003Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Pack predictions, labels, and attention_mask into the expected format\npred = (predictions, labels)\n#print(predictions)\nmetrics = compute_metrics(pred, attention_masks, QADataset(val_dataset))\n\n# Print the results\nprint(f\"F1 Macro: {metrics['f1_macro']}\")\nprint(f\"F1 Micro: {metrics['f1_micro']}\")\nprint(f\"IoU: {metrics['iou']}\")\n#print(f\"Report: {metrics['class_report']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:26:14.441766Z","iopub.execute_input":"2024-12-12T17:26:14.442090Z","iopub.status.idle":"2024-12-12T17:26:15.118751Z","shell.execute_reply.started":"2024-12-12T17:26:14.442062Z","shell.execute_reply":"2024-12-12T17:26:15.117778Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     34944\n           1       0.77      0.57      0.65      3911\n\n    accuracy                           0.94     38855\n   macro avg       0.86      0.77      0.81     38855\nweighted avg       0.93      0.94      0.94     38855\n\nF1 Macro: 0.8103671921170389\nF1 Micro: 0.9396988804529661\nIoU: 0.7108394632004718\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"QADataset(train_dataset)[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:13:42.327470Z","iopub.status.idle":"2024-12-12T17:13:42.327758Z","shell.execute_reply.started":"2024-12-12T17:13:42.327620Z","shell.execute_reply":"2024-12-12T17:13:42.327635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:33:33.825417Z","iopub.execute_input":"2024-11-26T11:33:33.825703Z","iopub.status.idle":"2024-11-26T11:33:33.832492Z","shell.execute_reply.started":"2024-11-26T11:33:33.825677Z","shell.execute_reply":"2024-11-26T11:33:33.831654Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'knowledge': 'Robert Albert Diaco (born February 19, 1973) is an American football coach and former player. He is currently the defensive coordinator at Nebraska. Nebraska also has the most wins and the highest winning percentage of any program over the last 50 years.',\n 'question': 'What position does Bob Diaco hold with the football team hat has the most wins and the highest winning percentage of any program over the last 50 years?',\n 'right_answer': 'defensive coordinator',\n 'hallucinated_answer': 'Bob Diaco is a quarterback.',\n 'hallucination': '\"quarterback\"'}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}