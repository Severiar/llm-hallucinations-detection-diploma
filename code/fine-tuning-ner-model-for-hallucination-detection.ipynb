{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9940562,"sourceType":"datasetVersion","datasetId":5844045}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\nimport torch\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-12T16:35:51.748201Z","iopub.execute_input":"2024-12-12T16:35:51.748944Z","iopub.status.idle":"2024-12-12T16:36:12.962304Z","shell.execute_reply.started":"2024-12-12T16:35:51.748914Z","shell.execute_reply":"2024-12-12T16:36:12.961408Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a7350fa7aa42feae46d19b08ff7143"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50181d027ac045d5a5bf0044c4cdbe97"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8533752dd1b400bab1a2d2ef5db3ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40007af433214210b3beb3cd918893d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8dd2a1931ef48aa9925d9219d0abf78"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import DistilBertTokenizerFast\n\nfrom sklearn.model_selection import train_test_split\n\n# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\nwith open(\"/kaggle/input/semeval-2025-task-3-mu-shroom-dataset/qa_data_output.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ val –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 9:1\ntrain_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤\nprint(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}\")\n\n# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\nwith open(\"train_data.json\", \"w\", encoding=\"utf-8\") as train_file:\n    json.dump(train_data, train_file, ensure_ascii=False, indent=4)\n\nwith open(\"val_data.json\", \"w\", encoding=\"utf-8\") as val_file:\n    json.dump(val_data, val_file, ensure_ascii=False, indent=4)\n\n# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\nimport torch\n\nimport torch\n\ndef create_dataset(data, tokenizer, max_length=512):\n    input_ids = []\n    attention_masks = []\n    labels_list = []\n    offset_mappings = []  # To store the offset mappings\n\n    for item in data:\n        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è question –∏ hallucinated_answer\n        text = 'query: ' + item[\"question\"] + \"\\n answer: \" + item[\"hallucinated_answer\"]\n        if 'hallucination' not in item.keys():\n            continue\n        hallucinations = item[\"hallucination\"].split('\\n')  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π\n        hallucinations = [h.strip(\"- \").strip('\"').strip() for h in hallucinations]  # –£–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–∞ –∏ –ø—Ä–æ–±–µ–ª—ã\n\n        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n        encoding = tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length,\n            return_offsets_mapping=True,\n            return_tensors=\"pt\"\n        )\n\n        # –ú–µ—Ç–∫–∏ (–≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é O: 0)\n        labels = [0] * max_length  # 0: O, 1: B-HALLUCINATION, 2: I-HALLUCINATION\n        offsets = encoding[\"offset_mapping\"][0].tolist()\n\n        # –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏\n        for hallucination in hallucinations:\n            hallucination_start = text.rfind(hallucination)\n            if hallucination_start == -1:\n                continue  # –ï—Å–ª–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ\n            hallucination_end = hallucination_start + len(hallucination)\n\n            for idx, (offset_start, offset_end) in enumerate(offsets):\n                if offset_start >= hallucination_start and offset_end <= hallucination_end:\n                    if labels[idx] == 0:\n                        labels[idx] = 1  # B-HALLUCINATION\n                    else:\n                        labels[idx] = 2  # I-HALLUCINATION\n\n        # –£—á–∏—Ç—ã–≤–∞–µ–º padding —Ç–æ–∫–µ–Ω—ã\n        for idx, mask_value in enumerate(encoding[\"attention_mask\"][0].tolist()):\n            if mask_value == 0:\n                labels[idx] = 0  # –ú–µ—Ç–∫–∞ –¥–ª—è padding —Ç–æ–∫–µ–Ω–æ–≤\n\n        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã, –º–∞—Å–∫—É –∏ –º–µ—Ç–∫–∏\n        input_ids.append(encoding[\"input_ids\"][0])\n        attention_masks.append(encoding[\"attention_mask\"][0])\n        labels_list.append(torch.tensor(labels))\n        offset_mappings.append(offsets)  # Save the offsets\n\n    return {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention_masks),\n        \"labels\": torch.stack(labels_list),\n        \"offset_mappings\": offset_mappings  # Return offset mappings\n    }\n\n\n\n# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞\ntrain_dataset = create_dataset(train_data, tokenizer)\nval_dataset = create_dataset(val_data, tokenizer)\n\n# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤\nprint(f\"Input IDs shape: {train_dataset['input_ids'].shape}\")\nprint(f\"Attention mask shape: {train_dataset['attention_mask'].shape}\")\nprint(f\"Labels shape: {train_dataset['labels'].shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-12T16:36:12.964002Z","iopub.execute_input":"2024-12-12T16:36:12.965041Z","iopub.status.idle":"2024-12-12T16:36:28.378971Z","shell.execute_reply.started":"2024-12-12T16:36:12.964998Z","shell.execute_reply":"2024-12-12T16:36:28.378070Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train size: 9000, Validation size: 1000\nInput IDs shape: torch.Size([8905, 512])\nAttention mask shape: torch.Size([8905, 512])\nLabels shape: torch.Size([8905, 512])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass QADataset(Dataset):\n    def __init__(self, dataset):\n        self.input_ids = dataset[\"input_ids\"]\n        self.attention_mask = dataset[\"attention_mask\"]\n        self.labels = dataset[\"labels\"]\n        self.offset_mappings = dataset[\"offset_mappings\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.input_ids) or idx < 0:\n            raise IndexError(f\"Invalid index: {idx}\")\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx],\n            \"labels\": self.labels[idx],\n            \"offset_mappings\": self.offset_mappings[idx]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:36:49.096663Z","iopub.execute_input":"2024-12-12T16:36:49.096997Z","iopub.status.idle":"2024-12-12T16:36:49.103326Z","shell.execute_reply.started":"2024-12-12T16:36:49.096969Z","shell.execute_reply":"2024-12-12T16:36:49.102409Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",          # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π\n    num_train_epochs=3,              # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n    per_device_train_batch_size=16,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n    per_device_eval_batch_size=16,   # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n    warmup_steps=500,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞\n    weight_decay=0.01,               # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n    logging_dir=\"./logs\",            # –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤\n    evaluation_strategy=\"epoch\",     # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n    save_strategy=\"epoch\",           # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n    logging_steps=10,                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤\n    load_best_model_at_end=True      # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=QADataset(train_dataset),\n    eval_dataset=QADataset(val_dataset)\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-12T16:36:52.615577Z","iopub.execute_input":"2024-12-12T16:36:52.616247Z","iopub.status.idle":"2024-12-12T16:36:53.927817Z","shell.execute_reply.started":"2024-12-12T16:36:52.616217Z","shell.execute_reply":"2024-12-12T16:36:53.926876Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:30:10.195092Z","iopub.execute_input":"2024-11-21T08:30:10.195790Z","iopub.status.idle":"2024-11-21T08:42:59.863695Z","shell.execute_reply.started":"2024-11-21T08:30:10.195757Z","shell.execute_reply":"2024-11-21T08:42:59.862887Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111301841111122, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a942848c81f44e3baafa4e0cde917afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241121_083018-zqx900mu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/huggingface/runs/zqx900mu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1671' max='1671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1671/1671 12:37, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.523400</td>\n      <td>0.537923</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.408100</td>\n      <td>0.512118</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.230100</td>\n      <td>0.676637</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1671, training_loss=0.4508122793130715, metrics={'train_runtime': 768.726, 'train_samples_per_second': 34.752, 'train_steps_per_second': 2.174, 'total_flos': 3490460678016000.0, 'train_loss': 0.4508122793130715, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model.save_pretrained(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:43:33.673866Z","iopub.execute_input":"2024-11-21T08:43:33.674252Z","iopub.status.idle":"2024-11-21T08:43:34.342172Z","shell.execute_reply.started":"2024-11-21T08:43:33.674211Z","shell.execute_reply":"2024-11-21T08:43:34.341052Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./trained_model/tokenizer_config.json',\n './trained_model/special_tokens_map.json',\n './trained_model/vocab.txt',\n './trained_model/added_tokens.json',\n './trained_model/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T08:44:54.396059Z","iopub.execute_input":"2024-11-21T08:44:54.396957Z","iopub.status.idle":"2024-11-21T08:45:03.005936Z","shell.execute_reply.started":"2024-11-21T08:44:54.396921Z","shell.execute_reply":"2024-11-21T08:45:03.005086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [62/62 00:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.5121184587478638, 'eval_runtime': 8.6005, 'eval_samples_per_second': 114.993, 'eval_steps_per_second': 7.209, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom datetime import datetime\n\n# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏\ncurrent_datetime = datetime.now()\n\ntraining_args = TrainingArguments(\n    run_name=f\"RUN {current_datetime}\",\n    output_dir=f\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,           # –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 500 —à–∞–≥–æ–≤\n    logging_steps=5,        # –õ–æ–≥–∏ –∫–∞–∂–¥—ã–µ 500 —à–∞–≥–æ–≤\n    save_strategy=\"steps\",    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ 500 —à–∞–≥–æ–≤\n    save_steps=500,\n    warmup_steps=500,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞\n    weight_decay=0.01,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    load_best_model_at_end=True,  # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ\n    #metric_for_best_model=\"f1_macro\",  # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n    #greater_is_better=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:28.987892Z","iopub.execute_input":"2024-12-12T16:37:28.988232Z","iopub.status.idle":"2024-12-12T16:37:29.017728Z","shell.execute_reply.started":"2024-12-12T16:37:28.988201Z","shell.execute_reply":"2024-12-12T16:37:29.016760Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.metrics import f1_score, jaccard_score\n\ndef compute_metrics(pred, attention_mask):\n    predictions, labels = pred\n    \n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏\n    predictions = np.argmax(predictions, axis=2)\n\n    # –£–±–∏—Ä–∞–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ attention_mask\n    true_predictions = [\n        [p for (p, m) in zip(prediction, mask) if m == 1]\n        for prediction, mask in zip(predictions, attention_mask)\n    ]\n    true_labels = [\n        [l for (l, m) in zip(label, mask) if m == 1]\n        for label, mask in zip(labels, attention_mask)\n    ]\n\n    # –ü–µ—Ä–µ–≤–æ–¥–∏–º —Å–ø–∏—Å–∫–∏ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã\n    true_predictions = [item for sublist in true_predictions for item in sublist]\n    true_labels = [item for sublist in true_labels for item in sublist]\n\n    # –ú–µ—Ç—Ä–∏–∫–∏\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:53.824971Z","iopub.execute_input":"2024-12-12T16:37:53.825643Z","iopub.status.idle":"2024-12-12T16:37:53.832055Z","shell.execute_reply.started":"2024-12-12T16:37:53.825609Z","shell.execute_reply":"2024-12-12T16:37:53.831109Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import Trainer\nimport wandb\nwandb.init(project='your_project_name', name='new_run_name')  # Set unique name for each run\n\ndel model\nmodel = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=QADataset(train_dataset),\n    eval_dataset=QADataset(val_dataset),\n    #compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:37:57.132395Z","iopub.execute_input":"2024-12-12T16:37:57.133077Z","iopub.status.idle":"2024-12-12T16:38:07.212528Z","shell.execute_reply.started":"2024-12-12T16:37:57.133042Z","shell.execute_reply":"2024-12-12T16:38:07.211870Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111276015555581, max=1.0)‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62eaefac118c4ddaadfc11f3081b6af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241212_163803-3ltlp30w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w' target=\"_blank\">new_run_name</a></strong> to <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w' target=\"_blank\">https://wandb.ai/sasha_levykin-m-v-lomonosovmoscow-state-university/your_project_name/runs/3ltlp30w</a>"},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:38:07.213741Z","iopub.execute_input":"2024-12-12T16:38:07.213980Z","iopub.status.idle":"2024-12-12T16:46:38.945640Z","shell.execute_reply.started":"2024-12-12T16:38:07.213940Z","shell.execute_reply":"2024-12-12T16:46:38.944858Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1114' max='1114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1114/1114 08:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.009800</td>\n      <td>0.011726</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.011000</td>\n      <td>0.010699</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1114, training_loss=0.05715645355451685, metrics={'train_runtime': 509.9673, 'train_samples_per_second': 34.924, 'train_steps_per_second': 2.184, 'total_flos': 2326973785344000.0, 'train_loss': 0.05715645355451685, 'epoch': 2.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Evaluate on the validation dataset\neval_dataloader = DataLoader(QADataset(val_dataset), batch_size=16)\n\n# Collect predictions and labels\npredictions = []\nlabels = []\nattention_masks = []\n\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        # Get batch data\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        label_ids = batch[\"labels\"].to(device)\n\n        # Make predictions\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions.append(logits.detach().cpu().numpy())\n        labels.append(label_ids.detach().cpu().numpy())\n        attention_masks.append(attention_mask.detach().cpu().numpy())\n\n# Combine batches into arrays\npredictions = np.concatenate(predictions, axis=0)\nlabels = np.concatenate(labels, axis=0)\nattention_masks = np.concatenate(attention_masks, axis=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:55:09.936116Z","iopub.execute_input":"2024-12-12T16:55:09.936785Z","iopub.status.idle":"2024-12-12T16:55:19.221274Z","shell.execute_reply.started":"2024-12-12T16:55:09.936752Z","shell.execute_reply":"2024-12-12T16:55:19.220580Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def compute_metrics(pred, attention_mask, dataset):\n    predictions, labels = pred\n\n    # Predicted labels\n    predictions = np.argmax(predictions, axis=2)\n\n    # Filter predictions and labels using attention_mask\n    true_predictions = []\n    true_labels = []\n\n    for i, (prediction, label, mask) in enumerate(zip(predictions, labels, attention_masks)):\n        # Filter by attention mask\n        filtered_preds = [p for p, m in zip(prediction, mask) if m == 1]\n        filtered_labels = [l for l, m in zip(label, mask) if m == 1]\n\n        # Find start index of 'answer: ' for further filtering\n        text = tokenizer.decode(dataset[i][\"input_ids\"], skip_special_tokens=True)\n        answer_start_index = text.find(\"answer: \") + len(\"answer: \")\n\n        # Apply additional filtering for tokens after 'answer: '\n        offsets = dataset[i][\"offset_mappings\"]\n        filtered_preds = [\n            p for p, (start, _) in zip(filtered_preds, offsets) if start >= answer_start_index\n        ]\n        filtered_labels = [\n            l for l, (start, _) in zip(filtered_labels, offsets) if start >= answer_start_index\n        ]\n\n        true_predictions.extend(filtered_preds)\n        true_labels.extend(filtered_labels)\n\n    # Compute metrics\n    print(set(true_labels), set(true_predictions))\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:14:51.804635Z","iopub.execute_input":"2024-12-12T17:14:51.805310Z","iopub.status.idle":"2024-12-12T17:14:51.813305Z","shell.execute_reply.started":"2024-12-12T17:14:51.805278Z","shell.execute_reply":"2024-12-12T17:14:51.812375Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, jaccard_score\n\ndef compute_metrics(pred, attention_mask, dataset):\n    predictions, labels = pred\n\n    # Predicted labels\n    predictions = np.argmax(predictions, axis=2)\n\n    # Filter predictions and labels using attention_mask\n    true_predictions = []\n    true_labels = []\n\n    for i, (prediction, label, mask) in enumerate(zip(predictions, labels, attention_mask)):\n        # Filter by attention mask\n        filtered_preds = [p for p, m in zip(prediction, mask) if m == 1]\n        filtered_labels = [l for l, m in zip(label, mask) if m == 1]\n\n        # Find start index of 'answer: ' for further filtering\n        text = tokenizer.decode(dataset[i][\"input_ids\"], skip_special_tokens=True)\n        answer_start_index = text.find(\"answer: \") + len(\"answer: \")\n\n        # Apply additional filtering for tokens after 'answer: '\n        offsets = dataset[i][\"offset_mappings\"]\n        filtered_preds = [\n            p for p, (start, _) in zip(filtered_preds, offsets) if start >= answer_start_index\n        ]\n        filtered_labels = [\n            l for l, (start, _) in zip(filtered_labels, offsets) if start >= answer_start_index\n        ]\n\n        true_predictions.extend(filtered_preds)\n        true_labels.extend(filtered_labels)\n\n    # Compute overall metrics\n    true_labels = [min(1, i) for i in true_labels]\n    f1_macro = f1_score(true_labels, true_predictions, average=\"macro\")\n    f1_micro = f1_score(true_labels, true_predictions, average=\"micro\")\n    iou = jaccard_score(true_labels, true_predictions, average=\"macro\")\n\n    # Detailed class-specific metrics\n    class_report = classification_report(true_labels, true_predictions, output_dict=True)\n    print(classification_report(true_labels, true_predictions))\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"iou\": iou,\n        \"class_report\": class_report,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:26:13.676674Z","iopub.execute_input":"2024-12-12T17:26:13.677516Z","iopub.status.idle":"2024-12-12T17:26:13.689853Z","shell.execute_reply.started":"2024-12-12T17:26:13.677467Z","shell.execute_reply":"2024-12-12T17:26:13.689003Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Pack predictions, labels, and attention_mask into the expected format\npred = (predictions, labels)\n#print(predictions)\nmetrics = compute_metrics(pred, attention_masks, QADataset(val_dataset))\n\n# Print the results\nprint(f\"F1 Macro: {metrics['f1_macro']}\")\nprint(f\"F1 Micro: {metrics['f1_micro']}\")\nprint(f\"IoU: {metrics['iou']}\")\n#print(f\"Report: {metrics['class_report']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:26:14.441766Z","iopub.execute_input":"2024-12-12T17:26:14.442090Z","iopub.status.idle":"2024-12-12T17:26:15.118751Z","shell.execute_reply.started":"2024-12-12T17:26:14.442062Z","shell.execute_reply":"2024-12-12T17:26:15.117778Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97     34944\n           1       0.77      0.57      0.65      3911\n\n    accuracy                           0.94     38855\n   macro avg       0.86      0.77      0.81     38855\nweighted avg       0.93      0.94      0.94     38855\n\nF1 Macro: 0.8103671921170389\nF1 Micro: 0.9396988804529661\nIoU: 0.7108394632004718\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"QADataset(train_dataset)[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:13:42.327470Z","iopub.status.idle":"2024-12-12T17:13:42.327758Z","shell.execute_reply.started":"2024-12-12T17:13:42.327620Z","shell.execute_reply":"2024-12-12T17:13:42.327635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:33:33.825417Z","iopub.execute_input":"2024-11-26T11:33:33.825703Z","iopub.status.idle":"2024-11-26T11:33:33.832492Z","shell.execute_reply.started":"2024-11-26T11:33:33.825677Z","shell.execute_reply":"2024-11-26T11:33:33.831654Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'knowledge': 'Robert Albert Diaco (born February 19, 1973) is an American football coach and former player. He is currently the defensive coordinator at Nebraska. Nebraska also has the most wins and the highest winning percentage of any program over the last 50 years.',\n 'question': 'What position does Bob Diaco hold with the football team hat has the most wins and the highest winning percentage of any program over the last 50 years?',\n 'right_answer': 'defensive coordinator',\n 'hallucinated_answer': 'Bob Diaco is a quarterback.',\n 'hallucination': '\"quarterback\"'}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}